---
title: "Cleaning & Modeling | Home Credit Default Risk"
output:
  html_document:
    toc: true
    toc_depth: 2
  pdf_document: default
date: "10/30/2024"
---

# Project Descriptions & Summaries

## Business Problem
Home Credit Group struggles to accurately assess the creditworthiness of clients with limited or no credit history, leading to missed opportunities and higher default rates. The goal is to improve financial inclusion by responsibly lending to underserved clients who are likely to repay.

## Analytic Problem
The task is to build a predictive model using demographic, transactional, and alternative data to assess credit risk more accurately for non-traditional borrowers. This model aims to reduce defaults and improve loan approvals, with success measured by AUC-ROC, precision, recall, and F1 score.

## Data Cleaning and Preparation Summary

**Exploration and Initial Cleaning:**

- Loaded and explored datasets, analyzing class imbalance and relationships between predictors and the TARGET variable.

**Handling Missing Data:**

- Removed columns with more than 50% missing values and applied median or mode imputation for essential features with moderate missing data.

**Outlier Handling:**

- Capped extreme outliers in income to the 99th percentile to minimize distortion.

**Feature Selection:**

- Removed low-correlation features and irrelevant columns to streamline the dataset.

**Final Checks:**

- Verified data integrity by checking for any remaining missing values or inconsistencies.

Some of the cleaning steps are in the "Cleaning Data" section below.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```
# Cleaning Data

## Task 1: Loading & exploring data

```{r }
library(caTools)

application_train <- read.csv("application_train.csv")
application_test <- read.csv("application_test.csv")
```

## Task 2: Analyzing the target variable and predictor relationships

```{r }
#Check the balance of the target variable.
table(application_train$TARGET)

# Proportion of target variable
prop.table(table(application_train$TARGET))

#Calculate the accuracy of a majority class predictor.
majority_class_accuracy <- max(table(application_train$TARGET)) / nrow(application_train)
majority_class_accuracy


#Visualizing Relationships
library(ggplot2)

ggplot(application_train, aes(x = AMT_INCOME_TOTAL, fill = as.factor(TARGET))) +
    geom_histogram(bins = 30, position = "dodge") +
    labs(title = "Income Distribution by Target", x = "Total Income", fill = "Target")

#  Memory cleanup to remove any intermediate objects not required later
rm(target_balance, target_proportion, majority_class_accuracy)
gc()  # Trigger garbage collection to free up memory
```
Target Variable: The dataset is highly imbalanced, with 92% of cases being non-defaults and 8% defaults. A simple majority class classifier would achieve 91.93% accuracy but would not identify any defaults.

Income Distribution: The AMT_INCOME_TOTAL variable is heavily skewed due to a few high-income outliers. Applying a log transformation improved interpretability, showing that defaults are distributed across income levels but slightly concentrated in lower and mid-income brackets.

Outliers: There are a few extreme outliers in income, which could distort models and visualizations. Limiting the x-axis range helps focus on the main distribution.

Number of Children: Most households have few children, and there is no strong relationship between the number of children and the likelihood of defaulting on a loan.

```{r }
# Renaming the TARGET column to target
colnames(application_train)[colnames(application_train) == "TARGET"] <- "target"

```
## Task 3: Data exploration and cleaning 

```{r }
library(skimr)
library(janitor)
library(dplyr)
library(tidyr)

application_train <- clean_names(application_train)

# Calculate the percentage of missing data for each column
missing_data <- application_train %>%
  summarise(across(everything(), ~sum(is.na(.)) / n() * 100)) %>%
  pivot_longer(cols = everything(), names_to = "column", values_to = "missing_percentage") %>%
  filter(missing_percentage > 50)

# Display columns with more than 50% missing data
print(missing_data)


rm(missing_data)
gc() # Final memory cleanup
```

These are the columns with more than 50% missing data that should be dropped, as they may not provide useful information and could negatively impact the model.

If the important columns have missing data, and we can't delete them, the step would be use median imputation for numerical variables and mode imputation for categorical variables to fill in missing values.

```{r }
# Impute missing values in ext_source_1 with the median
application_train$ext_source_1[is.na(application_train$ext_source_1)] <- 
  median(application_train$ext_source_1, na.rm = TRUE)
``` 
```{r }
# Drop columns (that aren't important) with high missing values
application_train <- application_train %>%
  select(-c(occupation_type, apartments_avg, basementarea_avg, 
            years_build_avg, commonarea_avg, elevators_avg))
```
```{r }
# List of columns to remove based on the previous selection
columns_to_remove <- c(
    'FLOORSMAX_MODE', 'FLOORSMAX_MEDI', 'FLOORSMAX_AVG', 'FLOORSMIN_MODE', 'FLOORSMIN_MEDI', 'FLOORSMIN_AVG',
    'NONLIVINGAPARTMENTS_MODE', 'NONLIVINGAPARTMENTS_MEDI', 'NONLIVINGAPARTMENTS_AVG',
    'APARTMENTS_MODE', 'APARTMENTS_MEDI', 'APARTMENTS_AVG',
    'BASEMENTAREA_MODE', 'BASEMENTAREA_MEDI', 'BASEMENTAREA_AVG',
    'COMMONAREA_MODE', 'COMMONAREA_MEDI', 'COMMONAREA_AVG',
    'ELEVATORS_MODE', 'ELEVATORS_MEDI', 'ELEVATORS_AVG',
    'ENTRANCES_MODE', 'ENTRANCES_MEDI', 'ENTRANCES_AVG',
    'YEARS_BEGINEXPLUATATION_MODE', 'YEARS_BEGINEXPLUATATION_MEDI', 'YEARS_BEGINEXPLUATATION_AVG',
    'YEARS_BUILD_MODE', 'YEARS_BUILD_MEDI', 'YEARS_BUILD_AVG',
    'LANDAREA_MODE', 'LANDAREA_MEDI', 'LANDAREA_AVG',
    'LIVINGAPARTMENTS_MODE', 'LIVINGAPARTMENTS_MEDI', 'LIVINGAPARTMENTS_AVG',
    'LIVINGAREA_MODE', 'LIVINGAREA_MEDI', 'LIVINGAREA_AVG',
    'NONLIVINGAREA_MODE', 'NONLIVINGAREA_MEDI', 'NONLIVINGAREA_AVG',
    'FONDKAPREMONT_MODE', 'HOUSETYPE_MODE', 'WALLSMATERIAL_MODE', 'EMERGENCYSTATE_MODE'
)

# Drop these columns from the traindata dataset
application_train <- application_train[, !(names(application_train) %in% columns_to_remove)]

rm(columns_to_remove)
gc()  # Final memory cleanup
```
## Task 4: Data integrity check

```{r }
# Identify rows where amt_income_total exceeds a high threshold (e.g., above 10 million)
outlier_rows <- application_train %>% filter(amt_income_total > 10000000)

# Show a summary of the selected columns with outliers
summary(outlier_rows %>% select(amt_income_total, amt_credit, amt_annuity))


# Memory cleanup
rm(outlier_rows,simplified_outlier_rows)  # Remove 'outlier_rows' to free up memory
gc()  # Run garbage collection to reclaim unused memory

```
For Client 1 (ID: 114967), since the income seems disproportionately high compared to the loan amount and this client defaulted, this might be an outlier worth capping. This could be an extreme case or potentially erroneous data, so capping it at the 99th percentile would reduce its influence while retaining the data.

For Clients 2 and 3 we will keep the values as they seem more consistent with the other loan-related variables (credit, annuity, goods price).

```{r }
# Calculate the 99th percentile for amt_income_total
quantile_99 <- quantile(application_train$amt_income_total, 0.99, na.rm = TRUE)

# Cap the value of amt_income_total for Client 1 (ID: 114967)
application_train$amt_income_total <- ifelse(application_train$amt_income_total > quantile_99, 
                                             quantile_99, 
                                             application_train$amt_income_total)

rm(quantile_99)
gc()  # Final memory cleanup
```
## Task 5: Correlations between predictors and target variable

```{r }
# Select numeric columns for correlation analysis
numeric_cols <- application_train %>% select_if(is.numeric)

# Compute correlation matrix for numeric columns, using 'complete.obs' to ignore missing values
cor_matrix <- cor(numeric_cols, use = "complete.obs")

# Display correlations of each feature with the target variable
cor_target <- cor_matrix['target', ]

# Sort correlations in descending order to highlight stronger relationships
cor_target_sorted <- sort(cor_target, decreasing = TRUE)

# Define the threshold for displaying correlations
threshold <- 0.05

# Filter correlations with target variable based on the threshold
cor_target_filtered <- cor_target_sorted[abs(cor_target_sorted) > threshold]

# Print the filtered correlations
print(cor_target_filtered)


# Memory cleanup
rm(numeric_cols, cor_matrix, cor_target, cor_target_sorted)  # Remove objects after use
gc()  # Run garbage collection to free memory
```
We will focus on ext_source_1, ext_source_2, and ext_source_3 because these seem to be the most promising features to predict default and should be included in the model.

```{r }
# List of columns to drop based on low correlation
columns_to_remove <- c(
  "sk_id_curr", "cnt_children", "amt_income_total", "amt_credit", "amt_annuity", 
  "amt_goods_price", "region_population_relative", "days_birth", "days_employed", 
  "days_registration", "days_id_publish", "flag_mobil", "flag_emp_phone", 
  "flag_work_phone", "flag_cont_mobile", "flag_phone", "flag_email", 
  "cnt_fam_members", "region_rating_client", "region_rating_client_w_city", 
  "hour_appr_process_start", "reg_region_not_live_region", "reg_region_not_work_region", 
  "live_region_not_work_region", "reg_city_not_live_city", "reg_city_not_work_city", 
  "live_city_not_work_city", "ext_source_1", "ext_source_2", "ext_source_3", 
  "years_beginexpluatation_avg", "entrances_avg", "floorsmax_avg", "floorsmin_avg", 
  "landarea_avg", "livingapartments_avg", "livingarea_avg", "nonlivingapartments_avg", 
  "nonlivingarea_avg", "apartments_mode", "basementarea_mode", "years_beginexpluatation_mode", 
  "years_build_mode", "commonarea_mode", "elevators_mode", "entrances_mode", 
  "floorsmax_mode", "floorsmin_mode", "landarea_mode", "livingapartments_mode", 
  "livingarea_mode", "nonlivingapartments_mode", "nonlivingarea_mode", 
  "apartments_medi", "basementarea_medi", "years_beginexpluatation_medi", 
  "years_build_medi", "commonarea_medi", "elevators_medi", "entrances_medi", 
  "floorsmax_medi", "floorsmin_medi", "landarea_medi", "livingapartments_medi", 
  "livingarea_medi", "nonlivingapartments_medi", "nonlivingarea_medi", 
  "totalarea_mode", "obs_30_cnt_social_circle", "def_30_cnt_social_circle", 
  "obs_60_cnt_social_circle", "def_60_cnt_social_circle", "days_last_phone_change", 
  "flag_document_2", "flag_document_3", "flag_document_4", "flag_document_5", 
  "flag_document_6", "flag_document_7", "flag_document_8", "flag_document_9", 
  "flag_document_10", "flag_document_11", "flag_document_12", "flag_document_13", 
  "flag_document_14", "flag_document_15", "flag_document_16", "flag_document_17", 
  "flag_document_18", "flag_document_19", "flag_document_20", "flag_document_21", 
  "amt_req_credit_bureau_hour", "amt_req_credit_bureau_day", "amt_req_credit_bureau_week", 
  "amt_req_credit_bureau_mon", "amt_req_credit_bureau_qrt", "amt_req_credit_bureau_year"
)

# Remove these columns from the dataset
application_train <- application_train[ , !(names(application_train) %in% columns_to_remove)]

rm(columns_to_remove)
gc()  # Final memory cleanup

```
```{r }
# Calculate the percentage of missing values in each column
missing_data <- colSums(is.na(application_train))
missing_percentage <- (missing_data / nrow(application_train)) * 100


print(missing_percentage)

rm(missing_percentage, missing_data)
gc()  # Final memory cleanup

```

**Remove Columns with High Missing Data**

```{r }
# Remove the 'own_car_age' column due to a high percentage of missing values
application_train <- application_train[ , !(names(application_train) %in% "own_car_age")]

```
# Modeling

```{r }
library(caret)
library(lattice)

# Convert target to a factor before upsampling
application_train$target <- as.factor(application_train$target)

# Now perform upsampling
balanced_data <- upSample(x = application_train[, -which(names(application_train) == "target")],
                          y = application_train$target, 
                          yname = "target")


```
## Step 1: Set up Cross-Validation

Evaluate how well a model will perform on unseen data. It helps check if the model is overfitting (too complex) or underfitting (too simple) by splitting the training data into smaller subsets.

```{r }
# Ensure the target variable is a factor with two levels
application_train$target <- as.factor(application_train$target)

```
```{r }
library(caret)
library(ggplot2)
library(lattice)

# Set up k-fold cross-validation with 3 folds
train_control <- trainControl(method = "cv", number = 3)

# Train logistic regression model with cross-validation
logit_cv_model <- train(target ~ ., data = application_train, method = "glm", family = binomial, trControl = train_control)

# Print the model results
print(logit_cv_model)

rm(logit_cv_model)
gc()  # Final memory cleanup
```
The model achieved 91.93% accuracy, but the Kappa statistic was 0.00012, indicating poor performance in distinguishing between the two classes. This suggests potential class imbalance issues.

## Step 2: Benchmark: Majority Class Classifier

```{r }
# Check the distribution of the target variable
table(application_train$target)
```
Model will always predict 0 (majority)

```{r }
# Create majority class prediction (0 is the majority class)
majority_class <- factor(rep(0, nrow(application_train)), levels = c(0, 1))

# Convert actual target values to factor with the same levels
actual_class <- factor(application_train$target, levels = c(0, 1))


# Calculate accuracy
accuracy <- sum(majority_class == actual_class) / length(actual_class)
print(paste("Accuracy of Majority Class Classifier:", accuracy))

# Confusion matrix
library(caret)
confusionMatrix(majority_class, actual_class)

rm(majority_class, actual_class)
gc()  # Final memory cleanup
```
- Accuracy: 91.93%. This means the classifier correctly predicts 91.93% of the data, which is the proportion of the majority class (class 0).

- Sensitivity (for class 0): 1.0000. This means that the classifier perfectly identified all the majority class (class 0) cases.

- Specificity (for class 1): 0.0000. This means that the classifier failed to identify any of the minority class (class 1) cases.

Since the Majority Class Classifier shows poor performance in detecting class 1, you should proceed with more sophisticated models, such as Logistic Regression, Decision Trees, or Random Forest, that can better identify both classes.

## Step 3: Fit Logistic Regression Models

**Basic Logistic Regression Model**

```{r }
# Load necessary libraries
library(caret)
library(pROC)

# Fit a logistic regression model with available predictors
logit_model_1 <- glm(target ~ code_gender + flag_own_car + flag_own_realty + name_income_type + name_education_type + name_family_status, 
                     data = application_train, family = binomial)

# Extract and print essential summary elements
cat("AIC:", logit_model_1$aic, "\n")
cat("Null Deviance:", logit_model_1$null.deviance, "\n")
cat("Residual Deviance:", logit_model_1$deviance, "\n")
cat("Number of Fisher Scoring Iterations:", logit_model_1$iter, "\n")
```
```{r }
# Predict probabilities for the logistic regression model
logit_probs <- predict(logit_model_1, type = "response")

# Convert probabilities to binary outcomes (using 0.5 threshold)
logit_preds <- ifelse(logit_probs > 0.5, 1, 0)

# Actual class labels
actual_class <- application_train$target

# Create a confusion matrix
conf_matrix <- table(Prediction = logit_preds, Actual = actual_class)

# Print the confusion matrix
print(conf_matrix)

# Calculate accuracy
accuracy <- mean(logit_preds == actual_class)
cat("Accuracy:", accuracy, "\n")

rm(logit_probs, logit_preds, conf_matrix)
gc()  # Final memory cleanup
```

- Many of the coefficients are statistically significant, especially for code_gender, flag_own_car, and several levels of name_education_type and name_family_status, indicating they are meaningful predictors of loan default.

- The model’s accuracy is 91.9%, which aligns with the accuracy observed in previous benchmarks (e.g., Majority Class Classifier).

- However, the AIC value (168770) suggests there is still room for improvement, and the model may be overfitting due to the inclusion of less impactful variables.


**Add Interaction Terms to the Logistic Regression Model**
```{r }
# Fit logistic regression model with interaction terms
logit_model_interaction <- glm(target ~ code_gender * flag_own_car + flag_own_realty * name_income_type +
                               name_education_type * name_family_status, 
                               family = binomial, data = application_train)

# Display only the AIC value for the model
cat("AIC for Logistic Model with Interaction Terms:", AIC(logit_model_interaction), "\n")

```

- The AIC of the model is 168758, which can be used to compare with other models (lower AIC is better).


**Evaluate Model Performance using AUC**
```{r }
# Get predicted probabilities
predicted_probs_interaction <- predict(logit_model_interaction, type = "response")

# Calculate AUC for the interaction model
roc_curve_interaction <- roc(application_train$target, predicted_probs_interaction)
auc(roc_curve_interaction)

rm(predicted_probs_interaction, roc_curve_interaction)
gc()  # Final memory cleanup
```
AUC 0.615 for your logistic regression model suggests that the model has modest predictive power, but it’s not performing exceptionally well. An AUC of 0.5 would mean random guessing, so 0.615 indicates the model is slightly better than random but still needs improvement.


**Fit Logistic Regression Models with Different Predictors**
```{r }
library(pROC)
library(broom)
library(dplyr)

# Fit a basic logistic regression model with selected predictors
logit_model_1 <- glm(target ~ code_gender + flag_own_car + flag_own_realty + 
                     name_income_type + name_education_type + name_family_status, 
                     data = application_train, family = binomial)

# Print key values from the model summary without all coefficients
cat("Logistic Regression Model Summary\n")
cat("--------------------------------------------------\n")
cat("Null Deviance: ", summary(logit_model_1)$null.deviance, "\n")
cat("Residual Deviance: ", summary(logit_model_1)$deviance, "\n")
cat("AIC: ", AIC(logit_model_1), "\n")
cat("Number of Fisher Scoring Iterations: ", summary(logit_model_1)$iter, "\n")
cat("--------------------------------------------------\n")


# Predict probabilities for the test data
predicted_probs_1 <- predict(logit_model_1, type = "response")

# Convert probabilities to binary predictions using 0.5 threshold
predictions_1 <- ifelse(predicted_probs_1 > 0.5, 1, 0)

# Calculate accuracy
accuracy_1 <- mean(predictions_1 == application_train$target)
print(paste("Accuracy of Model 1:", accuracy_1))

# Calculate AUC-ROC
library(pROC)
roc_1 <- roc(application_train$target, predicted_probs_1)
auc_1 <- auc(roc_1)
print(paste("AUC of Model 1:", auc_1))

rm(predicted_probs_1,predictions_1,accuracy_1, roc_1, auc_1 )
gc()  # Final memory cleanup

```

**Significant Variables:**

- code_genderM: Positive coefficient indicates that males are more likely to default.

-flag_own_carY: Negative coefficient suggests car ownership reduces the likelihood of default.

-name_education_type (various levels): Higher education types show significance in predicting defaults.

-name_family_status: Certain family statuses like Married, Separated, and Widow also significantly affect credit default predictions.

AUC-ROC: 0.614 — This suggests the model is only slightly better than random guessing (AUC of 0.5),


**Adding Interaction Terms**
```{r }
# Fit a logistic regression model with interaction terms
logit_model_2 <- glm(target ~ code_gender * flag_own_car + flag_own_realty * name_income_type + 
                     name_education_type * name_family_status, 
                     family = binomial, data = application_train)

# Predict probabilities using the logistic regression model
predicted_probabilities_2 <- predict(logit_model_2, type = "response")

# Convert probabilities to binary outcomes (cutoff 0.5)
predicted_classes_2 <- ifelse(predicted_probabilities_2 > 0.5, 1, 0)

# Calculate accuracy of the model
accuracy_2 <- mean(predicted_classes_2 == application_train$target)
cat("Accuracy of Model 2 with Interaction Terms:", accuracy_2, "\n")

# Calculate AUC for Model 2
library(pROC)
auc_2 <- roc(application_train$target, predicted_probabilities_2)
cat("AUC of Model 2 with Interaction Terms:", auc_2$auc, "\n")

# Final memory cleanup
rm(predicted_probabilities_2, predicted_classes_2)
gc()
```
**Accuracy:** The accuracy remains at approximately 91.93%, similar to the model without interaction terms.

**AUC (Area Under the Curve):** The AUC increased slightly to 0.615 (from 0.614 in the previous model without interaction terms). This small improvement suggests that the interaction terms may help slightly with model discrimination but not significantly.


**Handle Imbalanced Data**

**Upsampling the Minority Class**

```{r }
# Load required package
library(caret)

# First, identify the minority and majority classes
minority_class <- subset(application_train, target == 1)
majority_class <- subset(application_train, target == 0)

# Upsample the minority class to match the size of the majority class
set.seed(123)  # For reproducibility
upsampled_minority <- minority_class[sample(1:nrow(minority_class), nrow(majority_class), replace = TRUE), ]

# Combine the upsampled minority class with the majority class
upsampled_train <- rbind(majority_class, upsampled_minority)

# Check the distribution of the target variable after upsampling
table(upsampled_train$target)
```
```{r }
# Fit a logistic regression model with the upsampled data
upsampled_logit_model <- glm(target ~ code_gender * flag_own_car + flag_own_realty * name_income_type + 
                               name_education_type * name_family_status, 
                             family = binomial, data = upsampled_train)

# Make predictions on the upsampled training set
upsampled_prob <- predict(upsampled_logit_model, type = "response")
upsampled_pred <- ifelse(upsampled_prob > 0.5, 1, 0)

# Calculate accuracy
upsampled_accuracy <- mean(upsampled_pred == upsampled_train$target)
cat("Accuracy of Upsampled Model:", upsampled_accuracy, "\n")

# Calculate AUC
library(pROC)
upsampled_roc <- roc(upsampled_train$target, upsampled_prob)
upsampled_auc <- auc(upsampled_roc)
cat("AUC of Upsampled Model:", upsampled_auc, "\n")

# Final memory cleanup
rm(upsampled_roc, upsampled_auc, upsampled_prob, upsampled_pred)
gc()

```
The results after upsampling show that the accuracy of the model has decreased to 0.582, but the AUC is still relatively similar at 0.614. The lower accuracy is expected since we've balanced the classes, shifting from the majority class dominance.


**Downsampling the Majority Class**

```{r }
# Downsample the majority class to match the size of the minority class
set.seed(123)  # For reproducibility
downsampled_majority <- majority_class[sample(1:nrow(majority_class), nrow(minority_class), replace = FALSE), ]

# Combine the downsampled majority class with the minority class
downsampled_train <- rbind(downsampled_majority, minority_class)

# Check the distribution of the target variable after downsampling
table(downsampled_train$target)
```
```{r }
# Fit a logistic regression model with the downsampled data
downsampled_logit_model <- glm(target ~ code_gender * flag_own_car + flag_own_realty * name_income_type + 
                                 name_education_type * name_family_status, 
                               family = binomial, data = downsampled_train)

# Make predictions on the downsampled training set
downsampled_prob <- predict(downsampled_logit_model, type = "response")
downsampled_pred <- ifelse(downsampled_prob > 0.5, 1, 0)

# Calculate accuracy
downsampled_accuracy <- mean(downsampled_pred == downsampled_train$target)
cat("Accuracy of Downsampled Model:", downsampled_accuracy, "\n")

# Calculate AUC
library(pROC)
downsampled_roc <- roc(downsampled_train$target, downsampled_prob)
downsampled_auc <- auc(downsampled_roc)
cat("AUC of Downsampled Model:", downsampled_auc, "\n")

# Final memory cleanup
rm(downsampled_roc, downsampled_auc, downsampled_prob, downsampled_pred)
gc()

```

**Accuracy:** Both upsampling and downsampling have resulted in lower accuracy (around 58%), which is expected when balancing classes. The imbalance in the original data inflated accuracy due to the dominance of class 0.

**AUC:** The AUC remains around 0.614 in both cases, indicating that the model's ability to distinguish between classes is consistent.


## Step 4: Random Forest Implementation

```{r }
# Load necessary libraries
library(randomForest)
library(caret)
library(pROC)

# Train the Random Forest model
rf_model <- randomForest(
  target ~ code_gender + flag_own_car + flag_own_realty + 
    name_income_type + name_education_type + name_family_status, 
  data = balanced_data, 
  ntree = 200, 
  mtry = 3, 
  importance = TRUE
)

# Generate predictions
rf_pred_probs <- predict(rf_model, newdata = balanced_data, type = "prob")[, 2]
rf_pred_class <- ifelse(rf_pred_probs > 0.5, 1, 0)
```
```{r }
# Function to calculate key metrics for model comparison
evaluate_model <- function(true_labels, predicted_probs, predicted_class) {
  library(caret)
  library(pROC)
  
  # Confusion matrix to get accuracy, precision, recall, and F1 score
  conf_matrix <- confusionMatrix(factor(predicted_class, levels = c(0, 1)), factor(true_labels, levels = c(0, 1)))
  accuracy <- conf_matrix$overall['Accuracy']
  precision <- posPredValue(factor(predicted_class, levels = c(0, 1)), factor(true_labels, levels = c(0, 1)))
  recall <- sensitivity(factor(predicted_class, levels = c(0, 1)), factor(true_labels, levels = c(0, 1)))
  f1_score <- 2 * ((precision * recall) / (precision + recall))
  
  # Calculate AUC using the predicted probabilities
  roc_curve <- roc(true_labels, predicted_probs)
  auc_value <- auc(roc_curve)
  
  return(list(Accuracy = accuracy, AUC = auc_value, Precision = precision, Recall = recall, F1_Score = f1_score))
}

# Evaluate the Random Forest model
rf_results <- evaluate_model(balanced_data$target, rf_pred_probs, rf_pred_class)

# Optional: Store results for comparison with other models
if (!exists("model_results")) {
  model_results <- list()
}
model_results[['Random Forest']] <- rf_results

# Print out the results to verify
print(model_results[['Random Forest']])


rm(rf_pred_probs,rf_pred_clas,roc_curve, auc_value)
gc()  # Final memory cleanup
```
These results suggest the Random Forest model has some ability to detect the positive class but could benefit from improvements to increase both AUC and F1 Score. 


## Step 5: Gradient Boosting Implementation

```{r }
# Load necessary libraries
library(gbm)
library(pROC)
library(caret)

# Ensure target is correctly typed as numeric (0 and 1)
application_train$target <- as.numeric(as.character(application_train$target))
application_train$target <- ifelse(application_train$target == 1, 1, 0) # Ensure binary 0/1 values

# Convert predictors to factors if they are categorical
application_train$code_gender <- as.factor(application_train$code_gender)
application_train$flag_own_car <- as.factor(application_train$flag_own_car)
application_train$flag_own_realty <- as.factor(application_train$flag_own_realty)
application_train$name_income_type <- as.factor(application_train$name_income_type)
application_train$name_education_type <- as.factor(application_train$name_education_type)
application_train$name_family_status <- as.factor(application_train$name_family_status)

# Step 1: Fit the Gradient Boosting Model
gbm_model <- gbm(
  target ~ code_gender + flag_own_car + flag_own_realty + name_income_type + 
    name_education_type + name_family_status,
  data = application_train,
  distribution = "bernoulli",
  n.trees = 200,               # Reduced for simplicity
  interaction.depth = 3,       # Depth of each tree
  shrinkage = 0.05,            # Learning rate
  n.minobsinnode = 10,         # Minimum observations in terminal nodes
  verbose = FALSE
)
```
```{r }
# Step 2: Generate Predictions
gbm_pred <- predict(gbm_model, application_train, n.trees = 200, type = "response")  # Probability predictions
gbm_pred_class <- ifelse(gbm_pred > 0.5, 1, 0)   # Convert probabilities to binary class predictions

# Step 3: Evaluate the Model
gbm_results <- evaluate_model(application_train$target, gbm_pred, gbm_pred_class)

# Step 4: Print Results
print(gbm_results)

rm(gbm_pred,gbm_pred_class)
gc()  # Final memory cleanup
```
Overall, these results suggest that this model is performing well, especially in terms of precision and recall. High recall and precision together mean that the model is not only identifying positive cases accurately but also avoiding too many false positives.

**Re-run model**

```{r }
# Load necessary libraries
library(gbm)
library(pROC)
library(caret)

# Re-run the Gradient Boosting Model on application_train
gbm_model_simplified <- gbm(
  target ~ code_gender + flag_own_car + flag_own_realty + name_income_type + 
    name_education_type + name_family_status,
  data = application_train,
  distribution = "bernoulli",  # For binary classification
  n.trees = 200,  # Reduced number of trees for faster processing
  interaction.depth = 3,  # Tree depth
  shrinkage = 0.05,  # Learning rate
  n.minobsinnode = 10,  # Minimum observations per node
  verbose = FALSE
)
```
```{r }
# Predict using the simplified model on application_train
gbm_pred <- predict(gbm_model_simplified, application_train, n.trees = 200, type = "response")
gbm_pred_class <- ifelse(gbm_pred > 0.5, 1, 0)

# Check lengths to confirm consistency
cat("Length of Target (application_train):", length(application_train$target), "\n")
cat("Length of Predictions:", length(gbm_pred_class), "\n")

# Evaluate the model on application_train
if (length(application_train$target) == length(gbm_pred_class)) {
  gbm_results <- evaluate_model(application_train$target, gbm_pred, gbm_pred_class)
  print(gbm_results)
} else {
  cat("Error: Target and predictions still do not have the same length.\n")
}

rm(gbm_pred,gbm_pred_class)
gc()  # Final memory cleanup
```
- Accuracy and Precision have slightly increased, showing that the model is now marginally more accurate and precise in predicting positive cases.

- AUC has also improved very slightly, indicating a marginally better distinction between positive and negative cases.

- Recall remains perfect at 1, meaning the model still identifies all true positives.

- F1 Score has a tiny increase, reflecting the slight improvements in both precision and accuracy.

The changes are minimal and may not indicate significant differences in the model's real-world performance. However, these slight improvements suggest that the model is very consistent across runs, which is a positive indicator of stability.

  
# Models Comparison:

Objective: Compare the performance of all the models you’ve built (e.g., Logistic Regression, Random Forest, and Gradient Boosting).
Metrics to Use: Accuracy, AUC, Precision, Recall, and possibly F1-Score.
Result: Identify which model performs best for your problem.

**Step 1: Evaluate Each Logistic Model**

```{r }
# Assuming evaluate_model function is already defined

# Logistic Model 1
logistic_pred_probs_1 <- predict(logit_model_1, newdata = application_train, type = "response")
logistic_pred_class_1 <- ifelse(logistic_pred_probs_1 > 0.5, 1, 0)
logistic_results_1 <- evaluate_model(application_train$target, logistic_pred_probs_1, logistic_pred_class_1)

# Logistic Model 2
logistic_pred_probs_2 <- predict(logit_model_2, newdata = application_train, type = "response")
logistic_pred_class_2 <- ifelse(logistic_pred_probs_2 > 0.5, 1, 0)
logistic_results_2 <- evaluate_model(application_train$target, logistic_pred_probs_2, logistic_pred_class_2)

# Logistic Model with Interaction Terms
logistic_pred_probs_interaction <- predict(logit_model_interaction, newdata = application_train, type = "response")
logistic_pred_class_interaction <- ifelse(logistic_pred_probs_interaction > 0.5, 1, 0)
logistic_results_interaction <- evaluate_model(application_train$target, logistic_pred_probs_interaction, logistic_pred_class_interaction)

rm(logistic_pred_probs_1,logistic_pred_class_1,logistic_pred_probs_2, logistic_pred_class_2,logistic_pred_probs_interaction, logistic_pred_class_interaction)
gc()  # Final memory cleanup
```

**Step 2: Store Results in model_results**

```{r }
# Initialize model_results list if not already done
if (!exists("model_results")) {
  model_results <- list()
}

# Store each logistic model’s results
model_results[['Logistic Model 1']] <- logistic_results_1
model_results[['Logistic Model 2']] <- logistic_results_2
model_results[['Logistic Model Interaction']] <- logistic_results_interaction
model_results[['Random Forest']] <- rf_results
model_results[['Gradient Boosting']] <- gbm_results

```
**Step 3: Create the Comparison Table**

```{r }
# Create and display the comparison table
comparison_df <- data.frame(
  Model = c('Logistic Model 1', 'Logistic Model 2', 'Logistic Model Interaction', 'Random Forest', 'Gradient Boosting'),
  Accuracy = c(logistic_results_1$Accuracy, logistic_results_2$Accuracy, logistic_results_interaction$Accuracy, rf_results$Accuracy, gbm_results$Accuracy),
  AUC = c(logistic_results_1$AUC, logistic_results_2$AUC, logistic_results_interaction$AUC, rf_results$AUC, gbm_results$AUC),
  Precision = c(logistic_results_1$Precision, logistic_results_2$Precision, logistic_results_interaction$Precision, rf_results$Precision, gbm_results$Precision),
  Recall = c(logistic_results_1$Recall, logistic_results_2$Recall, logistic_results_interaction$Recall, rf_results$Recall, gbm_results$Recall),
  F1_Score = c(logistic_results_1$F1_Score, logistic_results_2$F1_Score, logistic_results_interaction$F1_Score, rf_results$F1_Score, gbm_results$F1_Score)
)

print(comparison_df)

rm(comparison_df)
gc()  # Final memory cleanup

```

## Summary of Comparison Results

**Logistic Models (Model 1, Model 2, Interaction):**

- Accuracy: These models have high accuracy (~91.9%), suggesting they perform well on the overall dataset.
- AUC: Around 0.614-0.615 for each model, indicating moderate performance in distinguishing between positive and negative cases.
- Precision: Consistent precision of ~0.919 across all three models, meaning they accurately classify positive cases when they predict a positive outcome.
- Recall: Perfect recall (1.0) for each model, capturing all true positives, which results in higher F1 scores.
- F1 Score: High F1 scores (~0.957), indicating that the models achieve a good balance between precision and recall, primarily due to their high recall.

**Random Forest:**

- Accuracy: Lower than the logistic models, at around 59%.
- AUC: 0.607, slightly lower than the logistic models but still above random guessing.
- Precision: Moderate precision of 0.596, showing reasonable accuracy in identifying true positives.
- Recall: Lower recall at 0.554, meaning it misses some positive cases, unlike the logistic models.
- F1 Score: F1 score of ~0.574, which is lower than the logistic models, indicating that this model has a less balanced performance.

**Gradient Boosting:**

- Accuracy: Similar to logistic models at ~91.9%, indicating strong overall performance.
- AUC: Highest among all models at 0.617, suggesting it has a slightly better capacity to distinguish between classes.
- Precision: Consistent with logistic models at ~0.919, showing high accuracy when predicting positive outcomes.
- Recall: Perfect recall (1.0), meaning it identifies all true positives.
- F1 Score: F1 score of ~0.957, indicating a good balance between precision and recall, similar to logistic models.


# Recommendations & Kaggle Score

**Gradient Boosting** has the highest accuracy and AUC, which suggests it's the most capable model, even though its low recall suggests it may be missing many true positives.

Gradient boosting performs well with Home Credit’s range of demographic, transactional, and alternative data sources, enabling a more accurate prediction of repayment ability.

**How It Addresses Project Goals**
- Reduces Loan Defaults: Gradient Boosting effectively identifies high-risk borrowers, helping minimize defaults.

- Supports Financial Inclusion: The model is designed to assess creditworthiness even for clients without traditional credit histories, aligning with Home Credit’s mission.

- Improves Loan Approvals: Its high accuracy ensures reliable borrowers are more likely to receive approvals, balancing financial inclusion with profitability.

**Kaggle Score** is: 0.5000 (Private score), 0.5000 (Public score)




# Test Set

## Data cleaning

```{r }
# Define the cleaning function to standardize between train and test sets
clean_data <- function(data) {
  # Specify columns to drop, as with the training set
  columns_to_drop <- c(
    "occupation_type", "apartments_avg", "basementarea_avg", "years_build_avg", 
    "commonarea_avg", "elevators_avg", "sk_id_curr", "cnt_children", "amt_income_total", 
    "amt_credit", "amt_annuity", "amt_goods_price", "region_population_relative", 
    "days_birth", "days_employed", "days_registration", "days_id_publish", "flag_mobil",
    "flag_emp_phone", "flag_work_phone", "flag_cont_mobile", "flag_phone", "flag_email",
    "cnt_fam_members", "region_rating_client", "region_rating_client_w_city", 
    "hour_appr_process_start", "reg_region_not_live_region", "reg_region_not_work_region",
    "live_region_not_work_region", "reg_city_not_live_city", "reg_city_not_work_city", 
    "live_city_not_work_city", "ext_source_1", "ext_source_2", "ext_source_3",
    "years_beginexpluatation_avg", "entrances_avg", "floorsmax_avg", "floorsmin_avg",
    "landarea_avg", "livingapartments_avg", "livingarea_avg", "nonlivingapartments_avg",
    "nonlivingarea_avg", "apartments_mode", "basementarea_mode", "years_beginexpluatation_mode",
    "years_build_mode", "commonarea_mode", "elevators_mode", "entrances_mode",
    "floorsmax_mode", "floorsmin_mode", "landarea_mode", "livingapartments_mode",
    "livingarea_mode", "nonlivingapartments_mode", "nonlivingarea_mode",
    "apartments_medi", "basementarea_medi", "years_beginexpluatation_medi",
    "years_build_medi", "commonarea_medi", "elevators_medi", "entrances_medi",
    "floorsmax_medi", "floorsmin_medi", "landarea_medi", "livingapartments_medi",
    "livingarea_medi", "nonlivingapartments_medi", "nonlivingarea_medi", "totalarea_mode"
  )
  data <- data[, !(names(data) %in% columns_to_drop)]
  return(data)
}

# Step 1: Apply the cleaning function to the test set
application_test_cleaned <- clean_data(application_test)

# Step 2: Add missing columns from the train set to the test set as NA values
for (col in setdiff(names(application_train), names(application_test_cleaned))) {
  application_test_cleaned[[col]] <- NA
}

# Step 3: Match factor levels in the test set to align with the training set
factor_cols <- names(Filter(is.factor, application_train))
application_test_cleaned[factor_cols] <- lapply(factor_cols, function(col) {
  factor(application_test_cleaned[[col]], levels = levels(application_train[[col]]))
})

# Step 4: Reorder columns to match the training set structure
application_test_cleaned <- application_test_cleaned[, names(application_train)]

# Check for alignment issues with factor levels
for (col in factor_cols) {
  if (any(is.na(application_test_cleaned[[col]]))) {
    most_common_level <- levels(application_train[[col]])[which.max(table(application_train[[col]]))]
    application_test_cleaned[[col]][is.na(application_test_cleaned[[col]])] <- most_common_level
  }
}

# Check final dimensions and column names
cat("Test set dimensions:", dim(application_test_cleaned), "\n")
print("Column names in test set:")
print(colnames(application_test_cleaned))

# Stop execution if the test set is empty
if (nrow(application_test_cleaned) == 0) {
  stop("Test set is empty.")
}

# Confirm structure alignment between train and test sets
if (!identical(names(application_train), names(application_test_cleaned))) {
  stop("Mismatch between training and test set columns.")
} else {
  cat("Test set structure is aligned with the training set.\n")
}

```
```{r }


```


## Modeling comparison

```{r }
# Assuming you already have trained models for the training set (logit_model_1, logit_model_2, logit_model_interaction, rf_model, gbm_model)

# Define evaluation function if not defined
evaluate_model <- function(true_labels, predicted_probs, predicted_classes) {
  library(pROC)
  
  # Check if target has both classes (0 and 1)
  if (length(unique(true_labels)) < 2) {
    warning("The test set target variable does not contain both levels (0 and 1). AUC cannot be calculated.")
    return(list(
      Accuracy = mean(predicted_classes == true_labels, na.rm = TRUE),
      AUC = NA,
      Precision = ifelse(any(predicted_classes == 1), sum(predicted_classes == 1 & true_labels == 1) / sum(predicted_classes == 1), NA),
      Recall = ifelse(any(true_labels == 1), sum(predicted_classes == 1 & true_labels == 1) / sum(true_labels == 1), NA),
      F1_Score = NA
    ))
  } else {
    roc_obj <- roc(true_labels, predicted_probs)
    auc_value <- auc(roc_obj)
    precision <- sum(predicted_classes == 1 & true_labels == 1) / sum(predicted_classes == 1)
    recall <- sum(predicted_classes == 1 & true_labels == 1) / sum(true_labels == 1)
    f1_score <- 2 * ((precision * recall) / (precision + recall))
    accuracy <- mean(predicted_classes == true_labels)
    return(list(
      Accuracy = accuracy,
      AUC = auc_value,
      Precision = precision,
      Recall = recall,
      F1_Score = f1_score
    ))
  }
}
```

```{r }

# Logistic Model 1 on test set
logistic_test_probs_1 <- predict(logit_model_1, application_test_cleaned, type = "response")
logistic_test_preds_1 <- ifelse(logistic_test_probs_1 > 0.5, 1, 0)
logistic_results_test_1 <- evaluate_model(application_test_cleaned$target, logistic_test_probs_1, logistic_test_preds_1)

# Logistic Model 2 on test set
logistic_test_probs_2 <- predict(logit_model_2, application_test_cleaned, type = "response")
logistic_test_preds_2 <- ifelse(logistic_test_probs_2 > 0.5, 1, 0)
logistic_results_test_2 <- evaluate_model(application_test_cleaned$target, logistic_test_probs_2, logistic_test_preds_2)

# Logistic Model with Interaction on test set
logistic_test_probs_interaction <- predict(logit_model_interaction, application_test_cleaned, type = "response")
logistic_test_preds_interaction <- ifelse(logistic_test_probs_interaction > 0.5, 1, 0)
logistic_results_test_interaction <- evaluate_model(application_test_cleaned$target, logistic_test_probs_interaction, logistic_test_preds_interaction)
```
```{r }
# Random Forest Model on test set
rf_test_probs <- predict(rf_model, application_test_cleaned, type = "prob")[,2]
rf_test_preds <- ifelse(rf_test_probs > 0.5, 1, 0)
rf_results_test <- evaluate_model(application_test_cleaned$target, rf_test_probs, rf_test_preds)

# Gradient Boosting Model on test set
gbm_test_probs <- predict(gbm_model, application_test_cleaned, type = "response")
gbm_test_preds <- ifelse(gbm_test_probs > 0.5, 1, 0)
gbm_results_test <- evaluate_model(application_test_cleaned$target, gbm_test_probs, gbm_test_preds)

# Create and display the comparison table for test set results
comparison_df_test <- data.frame(
  Model = c('Logistic Model 1', 'Logistic Model 2', 'Logistic Model Interaction', 'Random Forest', 'Gradient Boosting'),
  Accuracy = c(logistic_results_test_1$Accuracy, logistic_results_test_2$Accuracy, logistic_results_test_interaction$Accuracy, rf_results_test$Accuracy, gbm_results_test$Accuracy),
  AUC = c(logistic_results_test_1$AUC, logistic_results_test_2$AUC, logistic_results_test_interaction$AUC, rf_results_test$AUC, gbm_results_test$AUC),
  Precision = c(logistic_results_test_1$Precision, logistic_results_test_2$Precision, logistic_results_test_interaction$Precision, rf_results_test$Precision, gbm_results_test$Precision),
  Recall = c(logistic_results_test_1$Recall, logistic_results_test_2$Recall, logistic_results_test_interaction$Recall, rf_results_test$Recall, gbm_results_test$Recall),
  F1_Score = c(logistic_results_test_1$F1_Score, logistic_results_test_2$F1_Score, logistic_results_test_interaction$F1_Score, rf_results_test$F1_Score, gbm_results_test$F1_Score)
)

print(comparison_df_test)


```






